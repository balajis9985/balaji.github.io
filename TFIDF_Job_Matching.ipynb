{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "calvuW3fmhD2"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import accuracy_score\n",
        "import seaborn as sns\n",
        "from sklearn import datasets\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WPdKSk-OHW8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# verified\n",
        "df = pd.read_csv(\"VerifiedDataset (2).csv\",on_bad_lines='skip')\n",
        "\n",
        "# unverified\n",
        "df2 = pd.read_csv(\"stackoverflow_full (2).csv\",on_bad_lines='skip')\n",
        "\n",
        "# jobs\n",
        "linkedin_jobs = pd.read_csv(\"LinkedIn Jobs-Table 1 (2).csv\",on_bad_lines='skip')\n",
        "geton_jobs = pd.read_csv(\"GetOnBoard Jobs-Table 1 (2).csv\",on_bad_lines='skip')"
      ],
      "metadata": {
        "id": "sMp-EHnpm5Lz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vMnimEYqGHXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#===================Data Exploration and Cleaning===================="
      ],
      "metadata": {
        "id": "2qclmHZ4kOnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if dataframes are loaded\n",
        "if 'df' in globals() and 'df2' in globals():\n",
        "    print(\"Verified dataset columns:\", df.columns.tolist())\n",
        "    print(\"Unverified dataset columns:\", df2.columns.tolist())\n",
        "\n",
        "    # Check column exists before counting NaN\n",
        "    if \"HaveWorkedWith\" in df.columns:\n",
        "        nan_count = df['HaveWorkedWith'].isna().sum()\n",
        "        print(f'NaN count for verified data: {nan_count}')\n",
        "    else:\n",
        "        print(\"'HaveWorkedWith' not found in verified dataset\")\n",
        "\n",
        "    if \"HaveWorkedWith\" in df2.columns:\n",
        "        nan_count2 = df2['HaveWorkedWith'].isna().sum()\n",
        "        print(f'NaN count for unverified data: {nan_count2}')\n",
        "    else:\n",
        "        print(\"'HaveWorkedWith' not found in unverified dataset\")\n",
        "else:\n",
        "    print(\"Dataframes are not loaded. Please upload the CSVs first.\")\n"
      ],
      "metadata": {
        "id": "XSIJCQomrYQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['HaveWorkedWith'])\n",
        "df2 = df2.dropna(subset=['HaveWorkedWith'])\n",
        "\n",
        "nan_count = df[\"HaveWorkedWith\"].isna().sum()\n",
        "nan_count2 = df2[\"HaveWorkedWith\"].isna().sum()\n",
        "\n",
        "print(f'NaN count for verified data: {nan_count}')\n",
        "print(f'NaN count for unverified data: {nan_count2}')\n"
      ],
      "metadata": {
        "id": "rioxHjjcTfmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(linkedin_jobs.columns)\n",
        "print(geton_jobs.columns)"
      ],
      "metadata": {
        "id": "jIjZDmB7kX1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parsing!!!"
      ],
      "metadata": {
        "id": "qy6HV8NhMWwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Dataset before\n",
        "linkedin_jobs.head(2)"
      ],
      "metadata": {
        "id": "6hX6VJA_MbR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset before\n",
        "geton_jobs.head(2)"
      ],
      "metadata": {
        "id": "CKRJQ23-Mb16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stopwords english version\n",
        "nltk.download('stopwords')\n",
        "stop_words_en = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "WTQsOcO9Mb4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parsing for mostly english text\n",
        "def parse_text_linkedin(text):\n",
        "  text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text) # any joint words\n",
        "  text = text.lower() # lowercase\n",
        "  text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # non-ASCII characters\n",
        "  text = re.sub(r'\\d+', '', text)  # rid of all #\n",
        "  #text = re.findall(r'\\b[a-zA-Z0-9\\'-]+\\b', text)\n",
        "  words = re.findall(r'\\b[a-zA-Z\\'-]+\\b', text)  #  keep alphabetic characters and hyphenated words\n",
        "  words = [word for word in words if word not in stop_words_en] # remove stopwords english ver\n",
        "  text = ' '.join(words)\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "  return text\n",
        "\n",
        "# example to see if it works on english text\n",
        "text_main = \"€™s orders. â€¢ 34 Read physiciansâ€™ orders, measure arterial blood gases, and review patient information to assess patientâ€™s condition. â€¢ Explain treatment procedures to patients to gain cooperation and allay fears. â€¢ Monitor patientâ€™s physiological responses to therapy such as vital signs, arterial blood gases and blood chemistry changes and consult with physician if adverse reactions occur. â€¢ Administer therapeutic gases including nitrogen, nitric oxide, heliox, etc. â€¢ Enforce safety rules and ensure careful adherence to physiciansâ€™ orders. â€¢ Maintain charts that contain patient pertinent identification and therapy information. â€¢ Inspect, clean, test, and maintain respiratory therapy equipment to ensure equipment is functioning safely and efficiently and notify manager/supervisor when repairs are necessary. â€¢\"\n",
        "\n",
        "parse_text_linkedin(text_main)"
      ],
      "metadata": {
        "id": "y22UIGr0Mb6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# applying the parse_text function to the description column LINKEDIN DATASET\n",
        "linkedin_jobs['description'] = linkedin_jobs['description'].apply(lambda x: parse_text_linkedin(str(x)) if x is not None else \"\")\n",
        "\n",
        "# show the updated dataframe\n",
        "print(linkedin_jobs[['company_name','description']])"
      ],
      "metadata": {
        "id": "UFHaCSJTMb8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stopwords spanish version\n",
        "nltk.download('stopwords')\n",
        "stop_words_spa = set(stopwords.words('spanish'))"
      ],
      "metadata": {
        "id": "Hr9YDhm8Mb-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # spanish data set parse\n",
        "# def parse_text_getonjobs(text):\n",
        "#   text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text) # any joint words\n",
        "#   text = text.lower() # lowercase\n",
        "#   text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # non-ASCII characters\n",
        "#   text = re.sub(r'\\d+', '', text)  # rid of all numbers\n",
        "#   words = re.findall(r'\\b[a-zA-Z0-9\\'-]+\\b', text)\n",
        "#   #words = re.findall(r'\\b[a-zA-Z\\'-]+\\b', text)  #  keep alphabetic characters and hyphenated words\n",
        "#   words = [word for word in words if word not in stop_words_spa] # remove stopwords spanish ver\n",
        "#   text = ' '.join(words)\n",
        "#   text = re.sub(r'\\s+', ' ', text).strip()\n",
        "#   return text\n",
        "\n",
        "# # example to see if it works\n",
        "# text_main_s = \"â€¢ to and 67 allay fears. a to gases and MISIÓN la infraestructura Cloud para CMPC corresponde el stack tecnológico diferenciador que permitirá a la compañía alcanzar las metas de Innovación de acuerdo a la estrategia que acordó el directorio. Por lo cual, el Cloud Engineer deberá administrar, gestionar y controlar los contratos de administración Cloud directos e indirectos que CMPC posea con proveedores, velando por el adecuado cumplimiento de SLA´s y asegurando que el servicio entregado sea de excelencia y calidad; Así como administrar los diferentes Cloud provider que posea CMPC, eones, yaicios gestionados con terceros\"\n",
        "\n",
        "# parse_text_getonjobs(text_main_s)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# FIXED VER\n",
        "# spanish data set parse\n",
        "def parse_text_getonjobs(text):\n",
        "  text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text) # any joint words\n",
        "  text = text.lower() # lowercase\n",
        "  text = re.sub(r'\\d+', '', text)  # rid of all numbers\n",
        "\n",
        "  words = re.findall(r'\\b[a-záéíóúñ0-9\\'-]+\\b', text)  # kleep hyphenated words\n",
        "  words = [word for word in words if word not in stop_words_spa] # remove spanish stop words\n",
        "  text = ' '.join(words)\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "  return text\n",
        "\n",
        "# example to see if it works\n",
        "text_main_s = \"â€¢  to and allay fears. a to od gases and MISIÓN la infraestructura Cloud para CMPC corresponde el stack tecnológico diferenciador que permitirá a la compañía alcanzar las metas de Innovación de acuerdo a la estrategia que acordó el directorio. Por lo cual, el Cloud Engineer deberá administrar, gestionar y controlar los contratos de administración Cloud directos e indirectos que CMPC posea con proveedores, velando por el adecuado cumplimiento de SLA´s y asegurando que el servicio entregado sea de excelencia y calidad; Así como administrar los diferentes Cloud provider que posea CMPC, eones, yaicios gestionados con terceros\"\n",
        "\n",
        "parse_text_getonjobs(text_main_s)"
      ],
      "metadata": {
        "id": "t6D-Fji1N3FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# applying the parse_text function to the description column GETON_JOBS DATASET\n",
        "geton_jobs['Description'] = geton_jobs['Description'].apply(lambda x: parse_text_getonjobs(str(x)) if x is not None else \"\")\n",
        "\n",
        "# show the updated dataframe\n",
        "print(geton_jobs[['Company','Description']])"
      ],
      "metadata": {
        "id": "Y4iBVf39McA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after linkedin_jobs\n",
        "linkedin_jobs.head(2)"
      ],
      "metadata": {
        "id": "9IpsES9UNxFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after geton_jobs\n",
        "geton_jobs.head(2)"
      ],
      "metadata": {
        "id": "SKlf2qRlOKmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#===================TF-IDF Embedding====================="
      ],
      "metadata": {
        "id": "yZJYv-J_qvhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TFIDF Encoding on verified dataset\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "all_text = pd.concat([df[\"HaveWorkedWith\"],df2[\"HaveWorkedWith\"], linkedin_jobs[\"description\"], geton_jobs[\"Description\"]]).dropna()\n",
        "vectorizer.fit(all_text)\n",
        "\n",
        "# Transform each dataset separately using the same vectorizer\n",
        "verified_tfidf = vectorizer.transform(df[\"HaveWorkedWith\"])\n",
        "unverified_tfidf = vectorizer.transform(df2[\"HaveWorkedWith\"])\n",
        "\n",
        "linkedin_jobs_tfidf = vectorizer.transform(linkedin_jobs[\"description\"])\n",
        "geton_jobs_tfidf = vectorizer.transform(geton_jobs[\"Description\"])"
      ],
      "metadata": {
        "id": "DQ5T6pgiWSs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TFIDF Encoding on Jobs Datasets\n",
        "\n",
        "# Transform each dataset separately using the same vectorizer\n",
        "verified_tfidf = vectorizer.transform(df[\"HaveWorkedWith\"])\n",
        "unverified_tfidf = vectorizer.transform(df2[\"HaveWorkedWith\"])\n",
        "linkedin_jobs_tfidf = vectorizer.transform(linkedin_jobs[\"description\"])\n",
        "geton_jobs_tfidf = vectorizer.transform(geton_jobs[\"Description\"])"
      ],
      "metadata": {
        "id": "eyjCg6wcTvGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import getresuid\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "139s8YMkT6zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NEED TO RESHAPE SINCE DATASETS ARE NOT COMPATIBLE SHAPES\n",
        "\n",
        "#Compute cosine similarity between candidates and LinkedIn job descriptions\n",
        "sim_linkedin_ver = cosine_similarity(verified_tfidf, linkedin_jobs_tfidf)\n",
        "print(sim_linkedin_ver)\n",
        "sim_linkedin_unver = cosine_similarity(unverified_tfidf, linkedin_jobs_tfidf)\n",
        "print(sim_linkedin_unver)"
      ],
      "metadata": {
        "id": "4YhqK-q6lGwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute cosine similarity between candidates and GetOn job descriptions\n",
        "sim_geton_ver = cosine_similarity(verified_tfidf, geton_jobs_tfidf)\n",
        "print(sim_geton_ver)\n",
        "sim_geton_unver = cosine_similarity(unverified_tfidf, geton_jobs_tfidf)\n",
        "print(sim_geton_unver)"
      ],
      "metadata": {
        "id": "TX0eJjiAT-rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For each candidate, find the best match in LinkedIn\n",
        "bst_mtch_linkedin_ver = sim_linkedin_ver.argmax(axis=1)\n",
        "print(bst_mtch_linkedin_ver)\n",
        "bst_mtch_linkedin_unver = sim_linkedin_unver.argmax(axis=1)\n",
        "print(bst_mtch_linkedin_unver)"
      ],
      "metadata": {
        "id": "T8UBAiLgVPki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For each candidate, find the best match in GetOn\n",
        "bst_mtch_geton_ver = sim_geton_ver.argmax(axis=1)\n",
        "print(bst_mtch_geton_ver)\n",
        "bst_mtch_geton_unver = sim_geton_unver.argmax(axis=1)\n",
        "print(bst_mtch_geton_unver)"
      ],
      "metadata": {
        "id": "uEwoH2YqVQf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#===================Results====================="
      ],
      "metadata": {
        "id": "nhphEShqFBn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#===================LINKEDIN DATASET======================="
      ],
      "metadata": {
        "id": "P6eK3iZjObA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "most_similar_li = np.argmax(sim_linkedin_ver, axis=1)\n",
        "print(\"Most similar LinkedIn job for each verified candidate:\")\n",
        "print(most_similar_li)\n",
        "\n",
        "highest_similarity_li = np.max(sim_linkedin_ver, axis=1)\n",
        "print(\"Highest similarity values:\")\n",
        "print(highest_similarity_li)"
      ],
      "metadata": {
        "id": "lS2sZslJ_z6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(most_similar_li)\n",
        "linkedin_jobs_og = pd.read_csv(\"LinkedIn Jobs-Table 1.csv\",on_bad_lines='skip')\n",
        "\n",
        "most_similar_li_job_titles = df.iloc[most_similar_li]['HaveWorkedWith'].values\n",
        "print(most_similar_li_job_titles)"
      ],
      "metadata": {
        "id": "Mf8C9-e3A4ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Verified] Display the best job for each candidate along with the similarity score\n",
        "\n",
        "best_matches_li = np.argmax(sim_linkedin_ver, axis=1)\n",
        "print(best_matches_li)\n",
        "\n",
        "for i, match in enumerate(best_matches_li):\n",
        "\n",
        "  if sim_linkedin_ver[i][match] > .15:\n",
        "    print(f\"{i}, {match}\")\n",
        "\n",
        "    print(f\"Candidate {i} - {linkedin_jobs.loc[match, 'title']} with a similarity score of {sim_linkedin_ver[i][match]:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "agUR2sdMFrpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Unverified] Display the best job for each candidate along with the similarity score\n",
        "\n",
        "best_matches_li_unver = np.argmax(sim_linkedin_unver, axis=1)\n",
        "print(best_matches_li_unver)\n",
        "\n",
        "for i, match in enumerate(best_matches_li_unver):\n",
        "\n",
        "  if sim_linkedin_unver[i][match] > .10:\n",
        "    print(f\"{i}, {match}\")\n",
        "\n",
        "    print(f\"Candidate {i} - {linkedin_jobs.loc[match, 'title']} with a similarity score of {sim_linkedin_unver[i][match]:.2f}\")"
      ],
      "metadata": {
        "id": "kTEDqdJLay8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking if matches are the same for both unver and ver\n",
        "differences_li = best_matches_li - best_matches_li_unver\n",
        "differences_geton = [x for x in differences_li if x != 0]\n",
        "print(differences_geton)"
      ],
      "metadata": {
        "id": "O33MWjLBdXZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#comparing candidate skills and job title match\n",
        "for i, match in enumerate(best_matches_li):\n",
        "  if sim_linkedin_ver[i][match] > .10:\n",
        "    print(f\"Candidate Skills:\\t {df.loc[i, 'HaveWorkedWith']}\")\n",
        "    print(f\"Job: \\t\\t\\t {linkedin_jobs_og.loc[match, 'title']}\")\n"
      ],
      "metadata": {
        "id": "UAdZCZltN2rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#===================GETON DATASET======================="
      ],
      "metadata": {
        "id": "nTyUhHqFOj3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_similar_geton = np.argmax(sim_geton_ver, axis=1)\n",
        "print(\"Most similar Geton job for each verified candidate:\")\n",
        "print(most_similar_geton)\n",
        "\n",
        "highest_similarity_geton = np.max(sim_geton_ver, axis=1)\n",
        "print(\"Highest similarity values:\")\n",
        "print(highest_similarity_geton)"
      ],
      "metadata": {
        "id": "M7oXUgvJABN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(most_similar_geton)\n",
        "geton_jobs_og = pd.read_csv(\"GetOnBoard Jobs-Table 1 (2).csv\", on_bad_lines='skip')\n",
        "\n",
        "most_similar_geton_job_titles = df.iloc[most_similar_geton]['HaveWorkedWith'].values\n",
        "print(most_similar_geton_job_titles)"
      ],
      "metadata": {
        "id": "kr2V0DwGOwh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Verified] Display the best job for each candidate along with the similarity score\n",
        "\n",
        "best_matches_geton = np.argmax(sim_geton_ver, axis=1)\n",
        "print(geton_jobs.shape)\n",
        "\n",
        "\n",
        "for i, match in enumerate(best_matches_geton):\n",
        "  if sim_geton_ver[i][match] > .10:\n",
        "    print(f\"Candidate {i} - {geton_jobs.loc[match, 'Title']} with a similarity score of {sim_geton_ver[i][match]:.2f}\")\n"
      ],
      "metadata": {
        "id": "gETsxKrOO_0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Unverified] Display the best job for each candidate along with the similarity score\n",
        "\n",
        "best_matches_geton_unver = np.argmax(sim_geton_unver, axis=1)\n",
        "print(geton_jobs.shape)\n",
        "\n",
        "for i, match in enumerate(best_matches_geton_unver):\n",
        "  if sim_geton_unver[i][match] > .10:\n",
        "    print(f\"Candidate {i} - {geton_jobs.loc[match, 'Title']} with a similarity score of {sim_geton_unver[i][match]:.2f}\")\n"
      ],
      "metadata": {
        "id": "6YkxJlPFahV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking if matches are the same for both unver and ver\n",
        "differences_geton = best_matches_geton - best_matches_geton_unver\n",
        "differences_geton = [x for x in differences_geton if x != 0]\n",
        "print(differences_geton)"
      ],
      "metadata": {
        "id": "PT3znEXIcuXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# comparing candidate skills and job title match\n",
        "for i, match in enumerate(best_matches_geton):\n",
        "  if sim_geton_ver[i][match] > .12:\n",
        "    print(f\"Candidate Skills:\\t {df.loc[i, 'HaveWorkedWith']}\")\n",
        "    print(f\"Job: \\t\\t\\t {geton_jobs.loc[match, 'Title']}\")"
      ],
      "metadata": {
        "id": "Or4w-wLBPXT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "2SJ4S4b0lNdi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}